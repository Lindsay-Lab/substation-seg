{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.ops import masks_to_boxes\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir= r\"/scratch/kj1447/gracelab/dataset\"\n",
    "image_dir = os.path.join(data_dir, 'image_stack')\n",
    "mask_dir = os.path.join(data_dir, 'mask')\n",
    "image_filenames = os.listdir(image_dir)\n",
    "box_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image_dir = os.path.join(data_dir, 'image_stack_cropped')\n",
    "new_mask_dir = os.path.join(data_dir, 'mask_cropped')\n",
    "if not os.path.isdir(new_image_dir):\n",
    "    os.mkdir(new_image_dir)\n",
    "    print('image folder created')\n",
    "if not os.path.isdir(new_mask_dir):\n",
    "    os.mkdir(new_mask_dir)\n",
    "    print('mask folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26522"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0  completed.\n",
      "3.770454716838851  completed.\n",
      "7.540909433677702  completed.\n",
      "11.311364150516551  completed.\n",
      "15.081818867355404  completed.\n",
      "18.852273584194254  completed.\n",
      "22.622728301033103  completed.\n",
      "26.39318301787196  completed.\n",
      "30.163637734710807  completed.\n",
      "33.93409245154966  completed.\n",
      "37.70454716838851  completed.\n",
      "41.47500188522736  completed.\n",
      "45.245456602066206  completed.\n",
      "49.01591131890506  completed.\n",
      "52.78636603574392  completed.\n",
      "56.55682075258276  completed.\n",
      "60.327275469421615  completed.\n",
      "64.09773018626046  completed.\n",
      "67.86818490309932  completed.\n",
      "71.63863961993816  completed.\n",
      "75.40909433677702  completed.\n",
      "79.17954905361586  completed.\n",
      "82.95000377045471  completed.\n",
      "86.72045848729357  completed.\n",
      "90.49091320413241  completed.\n",
      "94.26136792097127  completed.\n",
      "98.03182263781012  completed.\n",
      "Image Distribution:  {5: 24593, 3: 442, 4: 673, 1: 331, 2: 483}\n"
     ]
    }
   ],
   "source": [
    "result={}\n",
    "for i, image_filename in enumerate(image_filenames):\n",
    "    image_path = os.path.join(image_dir, image_filename)\n",
    "    mask_filename = image_filename\n",
    "    mask_path = os.path.join(mask_dir, mask_filename)\n",
    "    image = np.load(image_path)['arr_0']\n",
    "    \n",
    "    if image.shape[0] in result:\n",
    "        result[image.shape[0]]+=1\n",
    "    else:\n",
    "        result[image.shape[0]]=1\n",
    "        \n",
    "    mask = np.load(mask_path)['arr_0']\n",
    "    mask[mask != 3] = 0\n",
    "    mask[mask == 3] = 1\n",
    "    \n",
    "#     print(\"MASK SUM\", mask.sum())\n",
    "    image = torch.from_numpy(image) #kx13x228x228\n",
    "    mask = torch.from_numpy(mask).float().unsqueeze(0) #1x228x228\n",
    "    if mask.sum()>0:#substation detected\n",
    "        # get bounding box coordinates for each mask\n",
    "        boxes = masks_to_boxes(mask) # n x 4 \n",
    "      \n",
    "        max_box = 0\n",
    "        #selecting just the first box\n",
    "        x1 = boxes[max_box,0].item()\n",
    "        y1 = boxes[max_box,1].item()\n",
    "        x2 = boxes[max_box, 2].item()\n",
    "        y2 = boxes[max_box, 3].item()\n",
    "\n",
    "        centers = [int(x1 + 0.5*np.abs(x1 - x2)), int(y1 + 0.5*np.abs(y1 - y2))]\n",
    "    else:\n",
    "        centers = [image.shape[2]//2, image.shape[3]//2]\n",
    "    offset = box_size//2\n",
    "\n",
    "    x1_new = centers[0] - offset\n",
    "    y1_new = centers[1] - offset\n",
    "    x2_new = centers[0] + offset\n",
    "    y2_new = centers[1] + offset\n",
    "\n",
    "    #checks: \n",
    "    if x1_new<0: \n",
    "        x2_new += np.abs(x1_new) #add additional width \n",
    "        x1_new=0\n",
    "    elif x2_new > mask.shape[2]: \n",
    "        x1_new = x1_new - np.abs(mask.shape[2] - x2_new) \n",
    "        x2_new = mask.shape[2] \n",
    "\n",
    "    if y1_new < 0: \n",
    "        y2_new += np.abs(y1_new)\n",
    "        y1_new = 0 \n",
    "    elif y2_new > mask.shape[1]:\n",
    "        y1_new = y1_new - np.abs(mask.shape[1] - y2_new) \n",
    "        y2_new = mask.shape[1] \n",
    "    \n",
    "    image_cropped = image[:,:, y1_new:y2_new, x1_new:x2_new]\n",
    "    mask_cropped = mask[:, y1_new:y2_new, x1_new:x2_new,]\n",
    "    mask_cropped = mask_cropped.squeeze(dim=0)\n",
    "    np.save(os.path.join(new_image_dir,image_filename),image_cropped)\n",
    "    np.save(os.path.join(new_mask_dir,mask_filename),mask_cropped)\n",
    "\n",
    "    \n",
    "    if i%1000==0:\n",
    "        print(i/len(image_filenames)*100, \" completed.\")\n",
    "print(\"Image Distribution: \",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "pytorch-example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
